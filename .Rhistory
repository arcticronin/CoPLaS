cat("\014")
rm(list = ls())
library(ISLR)
data <- College
? ISLR:: College
data <- data[, -1]
names(data)
dim(data)
attach(data)
cat("\014")
rm(list = ls())
library(ISLR)
data <- College
? ISLR:: College
library(ISLR)
data <- College
? ISLR:: College
data <- data[, -1]
names(data)
dim(data)
attach(data)
## (a)
set.seed(1)
train <- sample(c(TRUE, FALSE), nrow(data), replace = TRUE, prob = c(0.7, 0.3))
attach(data)
## (a)
set.seed(1)
cat("\014")
rm(list = ls())
library(ISLR)
data <- College
? ISLR:: College
data <- data[, -1]
names(data)
dim(data)
attach(data)
## (a)
set.seed(1)
train <- sample(c(TRUE, FALSE), nrow(data), replace = TRUE, prob = c(0.7, 0.3))
## (b) LS Regression
? lm
lm.fit <- lm(Apps ~ ., data = data, subset = train)
lm.pred <- predict(lm.fit, data[!train,])
lm.mse <- mean((lm.pred-Apps[!train])^2)
## (c) Ridge Regression
? model.matrix
x <- model.matrix (Apps ~ ., data)[, -1]
y <- Apps
library(glmnet)
? glmnet
## Before fitting the Ridge regression, analyse the variability of the data.
str(data)
library(pracma)
heatmap(cov(data[,-c(1:2)]), Colv = NA, Rowv = NA)
apply(data[, -1], 2, sd)
grid <- 10^seq(10, -2, length = 100)
length(grid)
...
ridge.mod <- glmnet( x, y, alpha = 0, lambda = grid)
library(glmnet)
install.packages("glmnet")
ridge.mod <- glmnet( x, y, alpha = 0, lambda = grid)
##install.packages("glmnet")
library(glmnet)
##install.packages("glmnet")
library(glmnet)
install.packages("glmnet")
library(glmnet)
library(glmnet)
install.packages("glmnet")
install.packages("glmnet")
install.packages("glmnet")
install.packages("Matrix")
install.package("foreach")
install.packages("foreach")
install.packages("shape")
install.packages("glmnet")
install.packages("glmnet")
library(glmnet)
? glmnet
## Before fitting the Ridge regression, analyse the variability of the data.
str(data)
library(pracma)
heatmap(cov(data[,-c(1:2)]), Colv = NA, Rowv = NA)
apply(data[, -1], 2, sd)
grid <- 10^seq(10, -2, length = 100)
length(grid)
ridge.mod <- glmnet( x, y, alpha = 0, lambda = grid)
## Before fitting the Ridge regression, analyse the variability of the data.
str(data)
library(pracma)
heatmap(cov(data[,-c(1:2)]), Colv = NA, Rowv = NA)
apply(data[, -1], 2, sd)
grid <- 10^seq(10, -2, length = 100)
length(grid)
ridge.mod <- glmnet( x, y, alpha = 0, lambda = grid)
cv.out <- cv.glmnet(x[train,], y[train,], alpha = 0)
cv.out <- cv.glmnet(x[train,], y[train,], alpha = 0)
x <- model.matrix (Apps ~ ., data)[, -1]
y <- Apps
install.packages("Matrix")
grid <- 10^seq(10, -2, length = 100)
length(grid)
ridge.mod <- glmnet( x, y, alpha = 0, lambda = grid)
## Which is the behaviour of the coefficient estimates when lambda increases?
coef.ridgemodel
ridge.mod <- glmnet( x, y, alpha = 0, lambda = grid)
## Which is the behaviour of the coefficient estimates when lambda increases?
coef.ridgemodel
## Which is the behaviour of the coefficient estimates when lambda increases?
coef.ridge.mod
ridge.pred <- predict(ridge.mod, newx = x[!train,])
ridge.pred <- predict(ridge.mod, newx = x[!train,], s = best.lambda.ridge)
cv.out <- cv.glmnet(x[train,], y[train,], alpha = 0)
cv.out <- cv.glmnet(x[train,], y[train,], alpha = 0)
best.lambda.ridge <- cv.out$lambda.min
best.lambda.ridge
log(best.lambda.ridge)
ridge.pred <- predict(ridge.mod, newx = x[!train,], s = best.lambda.ridge)
cat("\014")
rm(list = ls())
library(ISLR)
data <- College
? ISLR:: College
data <- data[, -1]
names(data)
dim(data)
attach(data)
## (a)
set.seed(1)
train <- sample(c(TRUE, FALSE), nrow(data), replace = TRUE, prob = c(0.7, 0.3))
## (b) LS Regression
? lm
lm.fit <- lm(Apps ~ ., data = data, subset = train)
lm.pred <- predict(lm.fit, data[!train,])
lm.mse <- mean((lm.pred-Apps[!train])^2)
## (c) Ridge Regression
? model.matrix
x <- model.matrix (Apps ~ ., data)[, -1]
y <- Apps
##install.packages("Matrix")
##install.packages("foreach")
##install.packages("shape")
##install.packages("glmnet")
library(glmnet)
? glmnet
## Before fitting the Ridge regression, analyse the variability of the data.
str(data)
library(pracma)
heatmap(cov(data[,-c(1:2)]), Colv = NA, Rowv = NA)
apply(data[, -1], 2, sd)
grid <- 10^seq(10, -2, length = 100)
length(grid)
ridge.mod <- glmnet( x, y, alpha = 0, lambda = grid)
## Which is the behaviour of the coefficient estimates when lambda increases?
coef.ridge.mod =
## How to choose the tuning parameter?
? cv.glmnet
cv.out <- cv.glmnet(x[train,], y[train,], alpha = 0)
cv.out <- cv.glmnet(x[train,-1], y[train,], alpha = 0)
cv.out <- cv.glmnet(x[train], y[train,], alpha = 0)
coef.ridge.mod = ridge.mod
coef.ridge.mod = ridge.mod$lambda
lasso.mod <- glmnet(x[train,],y[train,],alpha = grid)
dim(coef(lasso.mod))
lasso.mod <- glmnet(x[train,],y[train,],alpha = grid)
lasso.mod <- glmnet(x[train,], y[train,], alpha = grid, nlambda = 100)
lasso.mod <- glmnet(x[train,], y[train,], alpha = 1, lambda = grid, nlambda = 100)
install.packages("prioritylasso")
"ciap" + " + 10"
paste("1o" "20")
paste("1o", "20")
library(glmnet)
install.packages("glmnet")
library(survival)
library(glmnet)
library(prioritylasso)
setwd("/home/ronin/Dev/notebooks/thesis_notebook/")
data <- read.csv("DaniDatasets/preprocessed.csv")
# remove first column with patient ID
data <- data[-c(1)]
# Define the function
cox_lasso_model_with_offset <- function(df) {
# Check for necessary columns
required_cols <- c("time", "event")
if (!all(required_cols %in% names(df))) {
stop("Dataframe missing required columns: time, event")
}
# Prepare the survival object
y <- with(df, Surv(time, event))
# Prepare the matrix of predictors
predictors <- setdiff(names(df), required_cols)
x <- as.matrix(df[, predictors])
# Extract the offset
#offset_values <- df$offset
# Fit Cox model with Lasso using cross-validation
fit <- cv.glmnet(x, y, family="cox", alpha=1)
#fit <- cv.glmnet(x, y, family="cox", alpha=1, offset=offset_values)
# Best lambda and model fit
best_lambda <- fit$lambda.min
# best_fit <- glmnet(x, y, family="cox", alpha=1, lambda=best_lambda, offset=offset_values)
best_fit <- glmnet(x, y, family="cox", alpha=1, lambda=best_lambda)
# Extracting coefficients
coef_vector <- as.vector(coef(best_fit, s = best_lambda))
# handling intercept is not strictly recommended in COX, and not itnterpretable,
# so I'm leaving it out
# coef_df <- data.frame(coefficient = coef_vector, row.names = c("(Intercept)", predictors))
coef_df <- data.frame(coefficient = coef_vector, row.names = predictors)
# Return results
return(list(coef=coef_df, lambda=best_lambda, fit=best_fit))
}
model_results <- cox_lasso_model_with_offset(data)
set.seed(123) # for reproducibility
folds <- cut(seq(1, nrow(data)), breaks=5, labels=FALSE) # 5-fold CV
# Store results
cv_results <- data.frame(fold = integer(0), c_index = numeric(0))
for(i in 1:5){
# Split data into training and test sets
test_indices <- which(folds == i)
train_indices <- which(folds != i)
train_data <- data[train_indices, ]
test_data <- data[test_indices, ]
# Fit model on training data
y_train <- with(train_data, Surv(time, event))
x_train <- as.matrix(train_data[, setdiff(names(train_data), c("time", "event"))])
fit <- cv.glmnet(x_train, y_train, family="cox", alpha=1)
# Predict on test data
x_test <- as.matrix(test_data[, setdiff(names(test_data), c("time", "event"))])
predictions <- predict(fit, newx = x_test, s = "lambda.min")
# Calculate C-index for test data
c_index <- survConcordance(Surv(test_data$time, test_data$event) ~ predictions)$concordance
# Save results
cv_results <- rbind(cv_results, data.frame(fold = i, c_index = c_index))
}
# Calculate average C-index
mean_c_index <- mean(cv_results$c_index)
save.image("~/Dev/notebooks/thesis_notebook/mean_crossval_CIndex_lasso_glmnet.RData")
# Function to fit a Cox model with Ridge regularization on a range of columns
# used to score omics
cox_ridge_model <- function(df, start_col, end_col) {
# Check for necessary columns as before
required_cols <- c("time", "event")
if (!all(required_cols %in% names(df))) {
stop("Dataframe missing required columns: time, event")
}
# Validate column range
if (start_col < 1 || end_col > length(df)) {
stop("Invalid column range")
}
# Prepare the survival object
y <- with(df, Surv(time, event))
# Prepare the matrix of predictors
predictors <- names(df)[start_col:end_col]
x <- as.matrix(df[, predictors])
# Fit Cox model with Ridge using cross-validation
fit <- cv.glmnet(x, y, family="cox", alpha=0) # alpha=0 for Ridge
# Best lambda and model fit
best_lambda <- fit$lambda.min
best_fit <- glmnet(x, y, family="cox", alpha=0, lambda=best_lambda)
# Extracting coefficients
coef_vector <- as.vector(coef(best_fit, s = best_lambda))
coef_df <- data.frame(coefficient = coef_vector, row.names = predictors)
# Return results
return(list(coef=coef_df, lambda=best_lambda, fit=best_fit))
}
# Usage example:
clinical_score <- cox_ridge_model(data, start_col=1, end_col=4)
View(clinical_score)
calc_omic_importance <- function(coef_df) {
mean_sum_of_squares <- (sum(coef_df$coefficient^2))
/length(squared_coefs)
calc_omic_importance <- function(coef_df) {
mean_sum_of_squares <- (sum(coef_df$coefficient^2))/ length(squared_coefs)
return(mean_sum_of_squares)
}
clinical_ridge_fit <- cox_ridge_model(data, start_col=1, end_col=4)
calc_omic_importance <- function(coef_df) {
mean_sum_of_squares <- (sum(coef_df$coefficient^2))/ length(squared_coefs)
return(mean_sum_of_squares)
}
clinical_score <- calc_omic_importance(clinical_ridge_fit$coef)
calc_omic_importance <- function(coef_df) {
mean_sum_of_squares <- (sum(coef_df$coefficient^2))/ length(coef_df)
return(mean_sum_of_squares)
}
clinical_score <- calc_omic_importance(clinical_ridge_fit$coef)
indices_for_preprocessed <- read.csv("DaniDatasets/indices_for_preprocessed.csv")
View(indices_for_preprocessed)
clinical_ridge_fit <- cox_ridge_model(data, start_col=0, end_col=4)
View(data)
indices_for_preprocessed <- read.csv("DaniDatasets/indices_for_preprocessed.csv")
# Function to automate the fitting and scoring process
automate_ridge_fitting <- function(data, indices_for_preprocessed) {
indices <- as.numeric(indices_for_preprocessed$index)
# Initialize an empty list to store scores
scores <- list()
for (i in 1:(length(indices) - 1)) {
start_col <- indices[i] + 1
end_col <- indices[i + 1]
# Fit the Ridge model
ridge_fit <- cox_ridge_model(data, start_col, end_col)
# Calculate the importance score
score <- calc_omic_importance(ridge_fit$coef)
# Store the score
scores[[i]] <- score
}
names(scores) <- c("clinical", "rna", "methylation", "mirna", "cna_log2", "rppa", "alterations", "microbiome")
return(scores)
}
# Usage
scores <- automate_ridge_fitting(data, indices_for_preprocessed)
