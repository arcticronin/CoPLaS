}
# Prepare the survival object
y <- with(df, Surv(time, event))
# Prepare the matrix of predictors
predictors <- setdiff(names(df), required_cols)
x <- as.matrix(df[, predictors])
# Extract the offset
#offset_values <- df$offset
# Fit Cox model with Lasso using cross-validation
fit <- cv.glmnet(x, y, family="cox", alpha=1)
#fit <- cv.glmnet(x, y, family="cox", alpha=1, offset=offset_values)
# Best lambda and model fit
best_lambda <- fit$lambda.min
# best_fit <- glmnet(x, y, family="cox", alpha=1, lambda=best_lambda, offset=offset_values)
best_fit <- glmnet(x, y, family="cox", alpha=1, lambda=best_lambda)
# Extracting coefficients
coef_vector <- as.vector(coef(best_fit, s = best_lambda))
# handling intercept is not strictly recommended in COX, and not itnterpretable,
# so I'm leaving it out
# coef_df <- data.frame(coefficient = coef_vector, row.names = c("(Intercept)", predictors))
coef_df <- data.frame(coefficient = coef_vector, row.names = predictors)
# Return results
return(list(coef=coef_df, lambda=best_lambda, fit=best_fit))
}
model_results <- cox_lasso_model_with_offset(data)
set.seed(123) # for reproducibility
folds <- cut(seq(1, nrow(data)), breaks=5, labels=FALSE) # 5-fold CV
# Store results
cv_results <- data.frame(fold = integer(0), c_index = numeric(0))
# start crossval and store C-Index
for(i in 1:5){
# Split data into training and test sets
test_indices <- which(folds == i)
train_indices <- which(folds != i)
train_data <- data[train_indices, ]
test_data <- data[test_indices, ]
# Fit model on training data
y_train <- with(train_data, Surv(time, event))
x_train <- as.matrix(train_data[, setdiff(names(train_data), c("time", "event"))])
fit <- cv.glmnet(x_train, y_train, family="cox", alpha=1)
# Predict on test data
x_test <- as.matrix(test_data[, setdiff(names(test_data), c("time", "event"))])
predictions <- predict(fit, newx = x_test, s = "lambda.min")
# Calculate C-index for test data
c_index <- survConcordance(Surv(test_data$time, test_data$event) ~ predictions)$concordance
# Save results
cv_results <- rbind(cv_results, data.frame(fold = i, c_index = c_index))
}
# Calculate average C-index
mean_c_index <- mean(cv_results$c_index)
# Function to fit a Cox model with Ridge regularization on a range of columns
# used to score omics
cox_ridge_model <- function(df, start_col, end_col) {
# Check for necessary columns as before
required_cols <- c("time", "event")
if (!all(required_cols %in% names(df))) {
stop("Dataframe missing required columns: time, event")
}
# Validate column range
if (start_col < 1 || end_col > length(df)) {
stop("Invalid column range")
}
# Prepare the survival object
y <- with(df, Surv(time, event))
# Prepare the matrix of predictors
predictors <- names(df)[start_col:end_col]
x <- as.matrix(df[, predictors])
# Fit Cox model with Ridge using cross-validation
fit <- cv.glmnet(x, y, family="cox", alpha=0) # alpha=0 for Ridge
# Best lambda and model fit
best_lambda <- fit$lambda.min
best_fit <- glmnet(x, y, family="cox", alpha=0, lambda=best_lambda)
# Extracting coefficients
coef_vector <- as.vector(coef(best_fit, s = best_lambda))
coef_df <- data.frame(coefficient = coef_vector, row.names = predictors)
# Return results
return(list(coef=coef_df, lambda=best_lambda, fit=best_fit))
}
calc_omic_importance <- function(coef_df) {
mean(sum(coef_df$coefficient^2))
}
indices_for_preprocessed <- read.csv("DaniDatasets/indices_for_preprocessed.csv")
automate_ridge_fitting <- function(data, indices_for_preprocessed) {
indices <- as.numeric(indices_for_preprocessed$index)
scores <- list()
for (i in 1:(length(indices) - 1)) {
start_col <- indices[i] + 1
end_col <- indices[i + 1]
# Fit the Ridge model with columns
ridge_fit <- cox_ridge_model(data, start_col, end_col)
score <- calc_omic_importance(ridge_fit$coef)
scores[[i]] <- score
}
names(scores) <- c("clinical",
"rna",
"methylation",
"mirna",
"cna_log2",
"rppa",
"alterations",
"microbiome")
return(scores)
}
# calculating omic score
scores <- automate_ridge_fitting(data, indices_for_preprocessed)
scores2 <- automate_ridge_fitting(data, indices_for_preprocessed)
scores3 <- automate_ridge_fitting(data, indices_for_preprocessed)
# Define the number of iterations
num_iterations <- 5
# Initialize a list to store results from each iteration
all_scores <- list()
# Loop through the number of iterations
for (i in 1:num_iterations) {
# Run the function and store the scores
all_scores[[i]] <- automate_ridge_fitting(data, indices_for_preprocessed)
}
# Convert the list to a DataFrame
# Each element of the list becomes a column in the DataFrame
scores_df <- do.call(cbind, all_scores)
# Set column names as iterations
colnames(scores_df) <- paste("Iteration", 1:num_iterations)
# Set row names based on the names returned by the function
# Assuming the function returns the names consistently in each iteration
rownames(scores_df) <- names(all_scores[[1]])
# View the resulting DataFrame
print(scores_df)
# Plotting
ggplot(results_df, aes(x = Segment, y = Mean)) +
geom_bar(stat = "identity") +
geom_errorbar(aes(ymin = Mean - CI, ymax = Mean + CI), width = 0.2) +
theme_minimal() +
labs(title = "Ridge Fitting Scores with Confidence Intervals", x = "Segment", y = "Score")
library(ggplot)
install.packages("ggplot2")
library(ggplot)
library(ggplot2)
# Plotting
ggplot(results_df, aes(x = Segment, y = Mean)) +
geom_bar(stat = "identity") +
geom_errorbar(aes(ymin = Mean - CI, ymax = Mean + CI), width = 0.2) +
theme_minimal() +
labs(title = "Ridge Fitting Scores with Confidence Intervals", x = "Segment", y = "Score")
# Plotting
ggplot(scores_df, aes(x = Segment, y = Mean)) +
geom_bar(stat = "identity") +
geom_errorbar(aes(ymin = Mean - CI, ymax = Mean + CI), width = 0.2) +
theme_minimal() +
labs(title = "Ridge Fitting Scores with Confidence Intervals", x = "Segment", y = "Score")
library(reshape2)
install.packages("reshape2")
# confidence interval
library(dplyr)
install.packages("dplyr")
library(reshape2)
# confidence interval
library(dplyr)
# View the resulting DataFrame
print(scores_df)
# Reshape the data to long format for ggplot2
long_data <- melt(scores_df, variable.name = "Iteration", value.name = "Score")
long_data$Iteration <- as.factor(long_data$Iteration)
summary_data <- long_data %>%
group_by(variable) %>%
summarise(Mean = mean(Score),
SD = sd(Score),
SE = SD / sqrt(n()),
CI = qt(0.975, df = n() - 1) * SE)  # 95% CI
rlang::last_trace()
long_data
scores_df
# View the resulting DataFrame
print(scores_df.T)
# View the resulting DataFrame
print(T(scores_df))
# View the resulting DataFrame
print(t(scores_df))
# View the resulting DataFrame
print(t(scores_df))
# Reshaping the data for plotting
df_long <- reshape2::melt(scores_df)
# Calculating means and confidence intervals
df_summary <- df_long %>%
group_by(variable) %>%
summarize(
mean = mean(value),
se = sd(value) / sqrt(n()),
lower = mean - qt(1 - (0.05 / 2), df = n() - 1) * se,
upper = mean + qt(1 - (0.05 / 2), df = n() - 1) * se
)
# Plotting
ggplot(df_summary, aes(x = variable, y = mean)) +
geom_bar(stat = "identity", position = position_dodge(), color = "black", fill = "skyblue") +
geom_errorbar(aes(ymin = lower, ymax = upper), width = 0.2) +
ylab("Mean Values") +
ggtitle("Mean and Confidence Interval for Each Column") +
theme_minimal()
# View the resulting DataFrame
print(t(scores_df))
View(scores_df)
# confidence interval
library(dplyr)
# Calculating means and confidence intervals
df_summary <- df_long %>%
group_by(variable) %>%
summarize(
mean = mean(value),
se = sd(value) / sqrt(n()),
lower = mean - qt(1 - (0.05 / 2), df = n() - 1) * se,
upper = mean + qt(1 - (0.05 / 2), df = n() - 1) * se
)
# Calculating means and confidence intervals
df_summary <- df_long %>% group_by(variable) %>%
summarize(
mean = mean(value),
se = sd(value) / sqrt(n()),
lower = mean - qt(1 - (0.05 / 2), df = n() - 1) * se,
upper = mean + qt(1 - (0.05 / 2), df = n() - 1) * se
)
apply(scores_df, 2, var)
# Reshaping the data for plotting
df_long <- reshape2::melt(scores_df)
# Reshaping the data for plotting
df_long <- reshape2::melt(scores_df)
# Calculating means and confidence intervals
df_summary <- df_long %>% group_by(variable) %>%
summarize(
mean = mean(value),
se = sd(value) / sqrt(n()),
lower = mean - qt(1 - (0.05 / 2), df = n() - 1) * se,
upper = mean + qt(1 - (0.05 / 2), df = n() - 1) * se
)
typeof(scores_df)
typeof(data.frame(scores_df))
typeof(data.frame(scores_df))
typeof(data.frame(scores_df))
typeof(scores_df)
scores_df
typeof(scores_df[1])
View(scores_df)
# Convert the list to a DataFrame
# Each element of the list becomes a column in the DataFrame
scores_df <- do.call(cbind, all_scores)
typeof(scores_df[1])
View(scores_df)
typeof(scores_df)
typeof(scores_df)
# survival
library(survival)
library(glmnet)
library(prioritylasso)
# plotting
library(ggplot2)
library(reshape2)
# confidence interval
library(dplyr)
setwd("/home/ronin/Dev/notebooks/thesis_notebook/")
data <- read.csv("DaniDatasets/preprocessed.csv")
# remove first column with patient ID
data <- data[-c(1)]
# Define the function
cox_lasso_model_with_offset <- function(df) {
# Check for necessary columns
required_cols <- c("time", "event")
if (!all(required_cols %in% names(df))) {
stop("Dataframe missing required columns: time, event")
}
# Prepare the survival object
y <- with(df, Surv(time, event))
# Prepare the matrix of predictors
predictors <- setdiff(names(df), required_cols)
x <- as.matrix(df[, predictors])
# Extract the offset
#offset_values <- df$offset
# Fit Cox model with Lasso using cross-validation
fit <- cv.glmnet(x, y, family="cox", alpha=1)
#fit <- cv.glmnet(x, y, family="cox", alpha=1, offset=offset_values)
# Best lambda and model fit
best_lambda <- fit$lambda.min
# best_fit <- glmnet(x, y, family="cox", alpha=1, lambda=best_lambda, offset=offset_values)
best_fit <- glmnet(x, y, family="cox", alpha=1, lambda=best_lambda)
# Extracting coefficients
coef_vector <- as.vector(coef(best_fit, s = best_lambda))
# handling intercept is not strictly recommended in COX, and not itnterpretable,
# so I'm leaving it out
# coef_df <- data.frame(coefficient = coef_vector, row.names = c("(Intercept)", predictors))
coef_df <- data.frame(coefficient = coef_vector, row.names = predictors)
# Return results
return(list(coef=coef_df, lambda=best_lambda, fit=best_fit))
}
model_results <- cox_lasso_model_with_offset(data)
set.seed(123) # for reproducibility
folds <- cut(seq(1, nrow(data)), breaks=5, labels=FALSE) # 5-fold CV
# Store results
cv_results <- data.frame(fold = integer(0), c_index = numeric(0))
# start crossval and store C-Index
for(i in 1:5){
# Split data into training and test sets
test_indices <- which(folds == i)
train_indices <- which(folds != i)
train_data <- data[train_indices, ]
test_data <- data[test_indices, ]
# Fit model on training data
y_train <- with(train_data, Surv(time, event))
x_train <- as.matrix(train_data[, setdiff(names(train_data), c("time", "event"))])
fit <- cv.glmnet(x_train, y_train, family="cox", alpha=1)
# Predict on test data
x_test <- as.matrix(test_data[, setdiff(names(test_data), c("time", "event"))])
predictions <- predict(fit, newx = x_test, s = "lambda.min")
# Calculate C-index for test data
c_index <- survConcordance(Surv(test_data$time, test_data$event) ~ predictions)$concordance
# Save results
cv_results <- rbind(cv_results, data.frame(fold = i, c_index = c_index))
}
# Calculate average C-index
mean_c_index <- mean(cv_results$c_index)
# Function to fit a Cox model with Ridge regularization on a range of columns
# used to score omics
cox_ridge_model <- function(df, start_col, end_col) {
# Check for necessary columns as before
required_cols <- c("time", "event")
if (!all(required_cols %in% names(df))) {
stop("Dataframe missing required columns: time, event")
}
# Validate column range
if (start_col < 1 || end_col > length(df)) {
stop("Invalid column range")
}
# Prepare the survival object
y <- with(df, Surv(time, event))
# Prepare the matrix of predictors
predictors <- names(df)[start_col:end_col]
x <- as.matrix(df[, predictors])
# Fit Cox model with Ridge using cross-validation
fit <- cv.glmnet(x, y, family="cox", alpha=0) # alpha=0 for Ridge
# Best lambda and model fit
best_lambda <- fit$lambda.min
best_fit <- glmnet(x, y, family="cox", alpha=0, lambda=best_lambda)
# Extracting coefficients
coef_vector <- as.vector(coef(best_fit, s = best_lambda))
coef_df <- data.frame(coefficient = coef_vector, row.names = predictors)
# Return results
return(list(coef=coef_df, lambda=best_lambda, fit=best_fit))
}
calc_omic_importance <- function(coef_df) {
mean(sum(coef_df$coefficient^2))
}
indices_for_preprocessed <- read.csv("DaniDatasets/indices_for_preprocessed.csv")
automate_ridge_fitting <- function(data, indices_for_preprocessed) {
indices <- as.numeric(indices_for_preprocessed$index)
scores <- list()
for (i in 1:(length(indices) - 1)) {
start_col <- indices[i] + 1
end_col <- indices[i + 1]
# Fit the Ridge model with columns
ridge_fit <- cox_ridge_model(data, start_col, end_col)
score <- calc_omic_importance(ridge_fit$coef)
scores[[i]] <- score
}
names(scores) <- c("clinical",
"rna",
"methylation",
"mirna",
"cna_log2",
"rppa",
"alterations",
"microbiome")
return(scores)
}
# Define the number of iterations
num_iterations <- 5
# Initialize a list to store results from each iteration
all_scores <- list()
# Loop through the number of iterations
for (i in 1:num_iterations) {
# Run the function and store the scores
all_scores[[i]] <- automate_ridge_fitting(data, indices_for_preprocessed)
}
View(all_scores)
# Convert the list to a DataFrame
# Each element of the list becomes a column in the DataFrame
scores_df <- do.call(cbind, all_scores)
scores_df <- as.data.frame(scores_df)
scores_df <- as.data.frame(t(scores_df))
View(scores_df)
apply(scores_df, 2, var)
typeof(scores_df)
View(scores_df)
scores_df <- as.data.frame(t(scores_df))
typeof(scores_df)
num_iterations <- 3
all_scores <- list()
# Loop through the number of iterations
for (i in 1:num_iterations) {
# Run the function and ensure the output is a DataFrame or matrix
result <- automate_ridge_fitting(data, indices_for_preprocessed)
# If result is not a data.frame or matrix, convert it to one
if (!is.data.frame(result) && !is.matrix(result)) {
result <- as.data.frame(result)
}
all_scores[[i]] <- result
}
# Convert the list of data frames to a single data frame
# Bind the data frames by row
scores_df <- do.call(rbind, all_scores)
apply(scores_df, 2, var)
View(scores_df)
# Check the type of scores_df
typeof(scores_df)
class(scores_df)
View(scores_df)
apply(scores_df, 2, var)
# Calcolare media e intervallo di confidenza
df_summary <- scores_df %>%
pivot_longer(everything()) %>%
group_by(name) %>%
summarize(media = mean(value),
IC_inferiore = media - qt(0.975, df = n()-1) * sd(value) / sqrt(n()),
IC_superiore = media + qt(0.975, df = n()-1) * sd(value) / sqrt(n()))
library(tidyr)
install.packages("tidyr")
library(tidyr)
# Calcolare media e intervallo di confidenza
df_summary <- scores_df %>%
pivot_longer(everything()) %>%
group_by(name) %>%
summarize(media = mean(value),
IC_inferiore = media - qt(0.975, df = n()-1) * sd(value) / sqrt(n()),
IC_superiore = media + qt(0.975, df = n()-1) * sd(value) / sqrt(n()))
# Creare il grafico
ggplot(df_summary, aes(x = name, y = media)) +
geom_point() +
geom_errorbar(aes(ymin = IC_inferiore, ymax = IC_superiore), width = 0.2) +
theme_minimal() +
labs(x = "Variabile", y = "Media", title = "Media e Intervallo di Confidenza per ciascuna Variabile")
View(df_summary)
View(df_summary)
# Creare il grafico
ggplot(df_summary[-c(clinical)], aes(x = name, y = media)) +
geom_point() +
geom_errorbar(aes(ymin = IC_inferiore, ymax = IC_superiore), width = 0.2) +
theme_minimal() +
labs(x = "Variabile", y = "Media", title = "Media e Intervallo di Confidenza per ciascuna Variabile")
# Creare il grafico
ggplot(df_summary[,-c(clinical)], aes(x = name, y = media)) +
geom_point() +
geom_errorbar(aes(ymin = IC_inferiore, ymax = IC_superiore), width = 0.2) +
theme_minimal() +
labs(x = "Variabile", y = "Media", title = "Media e Intervallo di Confidenza per ciascuna Variabile")
# Creare il grafico
ggplot(df_summary[,-c('clinical')], aes(x = name, y = media)) +
geom_point() +
geom_errorbar(aes(ymin = IC_inferiore, ymax = IC_superiore), width = 0.2) +
theme_minimal() +
labs(x = "Variabile", y = "Media", title = "Media e Intervallo di Confidenza per ciascuna Variabile")
# Creare il grafico
ggplot(df_summary[-2,], aes(x = name, y = media)) +
geom_point() +
geom_errorbar(aes(ymin = IC_inferiore, ymax = IC_superiore), width = 0.2) +
theme_minimal() +
labs(x = "Variabile", y = "Media", title = "Media e Intervallo di Confidenza per ciascuna Variabile")
# Creare il grafico
ggplot(df_summary[-2,], aes(x = name, y = media)) +
geom_point() +
geom_errorbar(aes(ymin = IC_inferiore, ymax = IC_superiore), width = 0.2) +
theme_minimal() +
labs(x = "Omic", y = "Mean", title = "Mean and CI per Omic")
# Define the number of iterations
num_iterations <- 100
# Initialize a list to store results from each iteration
all_scores <- list()
# Loop through the number of iterations
for (i in 1:num_iterations) {
# Run the function and ensure the output is a DataFrame or matrix
result <- automate_ridge_fitting(data, indices_for_preprocessed)
# If result is not a data.frame or matrix, convert it to one
if (!is.data.frame(result) && !is.matrix(result)) {
result <- as.data.frame(result)
}
all_scores[[i]] <- result
}
# Convert the list of data frames to a single data frame
# Bind the data frames by row
scores_df <- do.call(rbind, all_scores)
# Check the type of scores_df
typeof(scores_df)
class(scores_df)
apply(scores_df, 2, var)
# Calcolare media e intervallo di confidenza
df_summary <- scores_df %>%
pivot_longer(everything()) %>%
group_by(name) %>%
summarize(media = mean(value),
IC_inferiore = media - qt(0.975, df = n()-1) * sd(value) / sqrt(n()),
IC_superiore = media + qt(0.975, df = n()-1) * sd(value) / sqrt(n()))
# Creare il grafico
ggplot(df_summary[-2,], aes(x = name, y = media)) +
geom_point() +
geom_errorbar(aes(ymin = IC_inferiore, ymax = IC_superiore), width = 0.2) +
theme_minimal() +
labs(x = "Omic", y = "Mean", title = "Mean and 95% CI per Omic")
# Creare il grafico
ggplot(df_summary[-2,], aes(x = name, y = media)) +
geom_point() +
geom_errorbar(aes(ymin = IC_inferiore, ymax = IC_superiore), width = 0.2) +
theme_minimal() +
labs(x = "Omic", y = "Score", title = "Mean and 95% CI of the Score of each Omic")
# Creare il grafico
ggplot(df_summary[-2,], aes(x = name, y = media)) +
geom_point() +
geom_errorbar(aes(ymin = IC_inferiore, ymax = IC_superiore), width = 0.2) +
theme_minimal() +
labs(x = "Omic", y = "Score", title = "Mean and 95% CI of the Score for each Omic")
